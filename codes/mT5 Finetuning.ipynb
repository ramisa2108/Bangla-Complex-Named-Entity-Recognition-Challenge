{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b856d5a-10d6-4e9d-bc63-6916b40fa85b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu114\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.14.1)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.7/site-packages (0.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.3.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.34.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.26.13)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2c41975-dd31-4c1b-9f71-fbf0c525bd7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Requirement already satisfied: omegaconf in /opt/conda/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: hydra-core in /opt/conda/lib/python3.7/site-packages (1.3.1)\n",
      "Requirement already satisfied: fairseq in /opt/conda/lib/python3.7/site-packages (0.10.2)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.97)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.7/site-packages (from omegaconf) (6.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.7/site-packages (from omegaconf) (4.9.3)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from hydra-core) (5.10.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from hydra-core) (23.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from fairseq) (1.13.1)\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.7/site-packages (from fairseq) (1.15.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from fairseq) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from fairseq) (4.64.1)\n",
      "Requirement already satisfied: cython in /opt/conda/lib/python3.7/site-packages (from fairseq) (0.29.15)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /opt/conda/lib/python3.7/site-packages (from fairseq) (2.3.1)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from fairseq) (0.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fairseq) (1.21.6)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu>=1.4.12->fairseq) (0.4.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from sacrebleu>=1.4.12->fairseq) (4.9.1)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu>=1.4.12->fairseq) (2.7.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi->fairseq) (2.19)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources->hydra-core) (3.11.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch->fairseq) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch->fairseq) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch->fairseq) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->fairseq) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch->fairseq) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->fairseq) (59.3.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->fairseq) (0.34.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mKeyring is skipped due to an exception: 'keyring.backends'\n",
      "Requirement already satisfied: seqeval in /opt/conda/lib/python3.7/site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (0.14.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install omegaconf hydra-core fairseq sentencepiece\n",
    "!pip3 install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c066b26-e203-4c5a-9cd9-b371633016a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mKeyring is skipped due to an exception: 'keyring.backends'\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install transformers\n",
    "!pip  -q install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1389f1ef-61b9-4995-8f90-a121c1528302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    \n",
    "    with open(filename) as file:\n",
    "        lines = [x.strip() for x in file.readlines()]\n",
    "    \n",
    "    tokens_list = []\n",
    "    ner_tags_list = []\n",
    "    \n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "    for l in lines:\n",
    "\n",
    "        if l == \"\":\n",
    "            tokens_list.append(tokens)\n",
    "            ner_tags_list.append(ner_tags)\n",
    "            tokens = []\n",
    "            ner_tags = []\n",
    "        else:\n",
    "            t, n = l.split(\" _ _ \")\n",
    "            tokens += [t]\n",
    "            ner_tags += [n]\n",
    "    \n",
    "    tokens_list.append(tokens)\n",
    "    ner_tags_list.append(ner_tags) \n",
    "    \n",
    "    return tokens_list, ner_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9619fcd-3e5b-4528-aa68-13cfa93582e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens_list, ner_tags_list = load_dataset('train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5501c502-3572-466c-9404-60141144801f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_tokens_list, dev_ner_tags_list = load_dataset('dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b1b70b7-a62d-4098-8be1-f1a94296b607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Sequence, Value, ClassLabel\n",
    "\n",
    "features = Features({\n",
    "  \"tokens\": Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
    "  \"ner_tags\": Sequence(feature=ClassLabel(names=[\"O\", \"B-PER\", \"B-LOC\", \"B-CORP\", \"B-GRP\", \"B-PROD\", \"B-CW\", \"I-PER\", \"I-LOC\", \"I-CORP\", \"I-GRP\", \"I-PROD\", \"I-CW\"], id=None), length=-1, id=None)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03cb7d35-9805-44d3-af5b-2b795c356805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_dict(\n",
    "  {\"tokens\": tokens_list, \"ner_tags\": ner_tags_list},\n",
    "  features=features\n",
    ")\n",
    "dev_ds = Dataset.from_dict(\n",
    "  {\"tokens\": dev_tokens_list, \"ner_tags\": dev_ner_tags_list},\n",
    "  features=features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c1ecab-b803-4ee5-8888-57c7b7040c58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tags = train_ds.features[\"ner_tags\"].feature\n",
    "\n",
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "\n",
    "# separate dataset into train dataset and validation dataset\n",
    "# ds = ds.train_test_split(test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "787415f9-bfb5-443c-82a2-5b23bf457aec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 5, 0, 0, 0],\n",
       " ['ভিনেগার', 'মাঝে', 'মাঝে', 'চাটনি', 'ব্যবহার', 'করা', 'হয়।'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_ds[1]['ner_tags'], dev_ds[1]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e27a65a0-51da-45ab-a315-c2d58b797590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d49a0cd91a40c896de47917dbb0458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e57aee6d4d4b048d2032340de0b81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def tokenize_and_align_labels(data):\n",
    "    text = [\"\".join(t) for t in data[\"tokens\"]]\n",
    "    tokenized_inputs = xlmr_tokenizer(text)\n",
    "\n",
    "    labels = []\n",
    "    for row_idx, label_old in enumerate(data[\"ner_tags\"]):\n",
    "        label_new = [[] for t in tokenized_inputs.tokens(batch_index=row_idx)]\n",
    "        for char_idx in range(len(data[\"tokens\"][row_idx])):\n",
    "            token_idx = tokenized_inputs.char_to_token(row_idx, char_idx)\n",
    "            if token_idx is not None:\n",
    "                label_new[token_idx].append(data[\"ner_tags\"][row_idx][char_idx])\n",
    "                if (tokenized_inputs.tokens(batch_index=row_idx)[token_idx] == \"▁\") and (data[\"ner_tags\"][row_idx][char_idx] != 0):\n",
    "                    label_new[token_idx+1].append(data[\"ner_tags\"][row_idx][char_idx])\n",
    "        label_new = list(map(lambda i : max(i, default=0), label_new))\n",
    "        labels.append(label_new)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# run conversion\n",
    "tokenized_train_ds = train_ds.map(\n",
    "  tokenize_and_align_labels,\n",
    "  remove_columns=[\"tokens\", \"ner_tags\"],\n",
    "  batched=True,\n",
    "  batch_size=128)\n",
    "tokenized_dev_ds = dev_ds.map(\n",
    "  tokenize_and_align_labels,\n",
    "  remove_columns=[\"tokens\", \"ner_tags\"],\n",
    "  batched=True,\n",
    "  batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3892974-2435-4e02-b05b-aae2c3ca6709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig\n",
    "from transformers.models.roberta.modeling_roberta import RobertaForTokenClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(\n",
    "  \"xlm-roberta-base\",\n",
    "  num_labels=tags.num_classes,\n",
    "  id2label=index2tag,\n",
    "  label2id=tag2index\n",
    ")\n",
    "model = (RobertaForTokenClassification\n",
    "         .from_pretrained(\"xlm-roberta-base\", config=xlmr_config)\n",
    "         .to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78392144-e246-4cc8-8213-a6b8a6c0d3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir = \"google-mt5-base-ner-ja\",\n",
    "  log_level = \"error\",\n",
    "  num_train_epochs = 20,\n",
    "  per_device_train_batch_size = 12,\n",
    "  per_device_eval_batch_size = 12,\n",
    "  evaluation_strategy = \"epoch\",\n",
    "  fp16 = True,\n",
    "  logging_steps = len(tokenized_train_ds),\n",
    "  push_to_hub = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23b7bc12-6778-46a0-ad70-b44dc3a1c226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "  xlmr_tokenizer,\n",
    "  return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c50b2b6-63cf-4198-886b-e10a461bd319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    preds = np.argmax(eval_arg.predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    y_true, y_pred = [], []\n",
    "    for b in range(batch_size):\n",
    "        true_label, pred_label = [], []\n",
    "        for s in range(seq_len):\n",
    "            if eval_arg.label_ids[b, s] != -100:  # -100 must be ignored\n",
    "                true_label.append(index2tag[eval_arg.label_ids[b][s]])\n",
    "                pred_label.append(index2tag[preds[b][s]])\n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(pred_label)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0954d3e3-2a20-4cf4-b688-f6fda5bcc95b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "  model = model,\n",
    "  args = training_args,\n",
    "  data_collator = data_collator,\n",
    "  compute_metrics = metrics_func,\n",
    "  train_dataset = tokenized_train_ds,\n",
    "  eval_dataset = tokenized_dev_ds,\n",
    "  tokenizer = xlmr_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b43d138a-6a1e-4d59-b798-1348f33c6980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25500' max='25500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25500/25500 1:45:20, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.145011</td>\n",
       "      <td>0.151229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.128096</td>\n",
       "      <td>0.267096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.327749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.104761</td>\n",
       "      <td>0.370889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111345</td>\n",
       "      <td>0.338645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.105062</td>\n",
       "      <td>0.405333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.112907</td>\n",
       "      <td>0.394472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.117604</td>\n",
       "      <td>0.396746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.119834</td>\n",
       "      <td>0.404066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131412</td>\n",
       "      <td>0.387742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.135665</td>\n",
       "      <td>0.424474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.139904</td>\n",
       "      <td>0.403990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.148914</td>\n",
       "      <td>0.410448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.154772</td>\n",
       "      <td>0.418634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.165502</td>\n",
       "      <td>0.425584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.172523</td>\n",
       "      <td>0.431861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.174989</td>\n",
       "      <td>0.418260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.179987</td>\n",
       "      <td>0.429907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.184685</td>\n",
       "      <td>0.431494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.186174</td>\n",
       "      <td>0.433437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25500, training_loss=0.052216264313342524, metrics={'train_runtime': 6321.4147, 'train_samples_per_second': 48.407, 'train_steps_per_second': 4.034, 'total_flos': 9997567081562448.0, 'train_loss': 0.052216264313342524, 'epoch': 20.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48339fa3-7929-438c-9113-9605d823f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1d332-6b55-4ff2-9f43-fff66698ab4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_tokens_list, test_ner_tags_list = load_dataset('bn_test.conll')\n",
    "test_ds = Dataset.from_dict(\n",
    "  {\"tokens\": test_tokens_list, \"ner_tags\": test_ner_tags_list},\n",
    "  features=features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e936e44-5860-45b6-b675-3a4f0c442af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# save fine-tuned model in local\n",
    "os.makedirs(\"./trained_ner_classifier_jp\", exist_ok=True)\n",
    "if hasattr(trainer.model, \"module\"):\n",
    "    trainer.model.module.save_pretrained(\"./trained_ner_classifier_jp\")\n",
    "else:\n",
    "    trainer.model.save_pretrained(\"./trained_ner_classifier_jp\")\n",
    "\n",
    "# load from the saved checkpoint\n",
    "xlmr_config = AutoConfig.from_pretrained(\n",
    "  \"xlm-roberta-base\",\n",
    "  num_labels=tags.num_classes,\n",
    "  id2label=index2tag,\n",
    "  label2id=tag2index\n",
    ")\n",
    "model = (RobertaForTokenClassification\n",
    "         .from_pretrained(\"./trained_ner_classifier_jp\", config=xlmr_config)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a9ba5-344a-414c-b161-3a4e7d109c78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# create dataset for prediction\n",
    "sample_encoding = xlmr_tokenizer(\n",
    "    # [\n",
    "    #     \"উপরন্তু, রুটটি ব্যবসায়িক রুট এর মানদণ্ড পূরণ করেনি।\",\n",
    "    #     \"উপরন্তু, রুটটি ব্যবসায়িক রুট এর মানদণ্ড পূরণ করেনি।\",\n",
    "    # ], \n",
    "    [\" \".join(test_ds['tokens'][i]) for i in range(len(test_ds['tokens']))],\n",
    "    truncation=True, max_length=512)\n",
    "sample_dataset = Dataset.from_dict(sample_encoding)\n",
    "sample_dataset = sample_dataset.with_format(\"torch\")\n",
    "\n",
    "# predict\n",
    "sample_dataloader = DataLoader(sample_dataset, batch_size=1)\n",
    "tokens = []\n",
    "labels = []\n",
    "for batch in sample_dataloader:\n",
    "  # predict\n",
    "    with torch.no_grad():\n",
    "        output = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device))\n",
    "    predicted_label_id = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
    "    # create output\n",
    "    tokens.append(xlmr_tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][0]))\n",
    "    labels.append([index2tag[i] for i in predicted_label_id[0]])\n",
    "\n",
    "# show the first result\n",
    "pd.DataFrame([tokens[0], labels[0]], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9750abba-8a7b-45e5-b19b-6c00606380c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_labels = []\n",
    "for i in range(len(labels)):\n",
    "    ml = []\n",
    "    for j in range(1, len(labels[i]), 1):\n",
    "        if tokens[i][j].startswith('▁'):\n",
    "            ml += [tag2index[labels[i][j]]]\n",
    "    merged_labels += [ml]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79452f32-44e8-401d-876d-e6f39da9aa71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_labels[:3], test_ds['ner_tags'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae3f9c-8f8a-4129-bca5-236142fba7bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ds['tokens'][1], test_ds['ner_tags'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d36f7e-f4fb-4669-88cf-91749d33092c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211a1dc-44e8-48b7-a8fe-f9110c7fa066",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dev_pred_labels.txt\", \"w\") as file:\n",
    "    for ml in merged_labels:\n",
    "        for x in ml:\n",
    "            file.write(x + \"\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9af9e-0d8b-4564-b663-3c5fa02d444d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "deep_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "de922fe3d48517865d28c852e8886e6823d1d88cf88f9c59e21f6a688fe659de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
